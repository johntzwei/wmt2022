{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e39d4e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import scipy\n",
    "from statsmodels.stats.power import TTestIndPower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a150f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUBLIC_RELEASE_PATH = \"C:/Users/t-johnnywei/Documents/GitHub/ToShipOrNotToShip\\public_release\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7febbd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(use_cache=True):\n",
    "    cache_filename = \"data.pickle\"\n",
    "    data = defaultdict(dict)\n",
    "    if use_cache and os.path.isfile(cache_filename):\n",
    "        with open(cache_filename, 'rb') as handle:\n",
    "            data = pickle.load(handle)\n",
    "    else:\n",
    "        _, campaigns_list, _ = next(os.walk(PUBLIC_RELEASE_PATH))\n",
    "        counter = 1\n",
    "        for campaign in campaigns_list:\n",
    "            if campaign not in data:\n",
    "                data[campaign] = defaultdict(dict)\n",
    "            for _, _, systems_list in os.walk(f\"{PUBLIC_RELEASE_PATH}/{campaign}\"):\n",
    "                for system in systems_list:\n",
    "                    if system not in data[campaign]:\n",
    "                        data[campaign][system] = defaultdict(dict)\n",
    "                    print(f\"Loading {counter}/{len(campaigns_list)} campaign\")\n",
    "                    xls = pd.ExcelFile(f\"{PUBLIC_RELEASE_PATH}/{campaign}/{system}\")\n",
    "                    for datatype in xls.sheet_names:\n",
    "                        if datatype in [\"hum_annotations\",\n",
    "                                        \"full_test\"]:\n",
    "                            data[campaign][system][datatype] = pd.read_excel(\n",
    "                                xls, datatype)\n",
    "                        else:\n",
    "                            df = pd.read_excel(xls, datatype)\n",
    "                            # transform to dictionary\n",
    "                            df_dict = df.set_index(\"Unnamed: 0\").transpose()\n",
    "                            df_dict = df_dict.iloc[0].to_dict()\n",
    "                            data[campaign][system][datatype] = df_dict\n",
    "                counter += 1\n",
    "\n",
    "        # save the cache data\n",
    "        if use_cache:\n",
    "            with open(cache_filename, 'wb') as handle:\n",
    "                pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(\"Annotated data loaded\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6211ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated data loaded\n"
     ]
    }
   ],
   "source": [
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0884dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs():\n",
    "    for (k, v) in data.items():\n",
    "        for i, j in itertools.combinations(v, 2):\n",
    "            yield (v[i]['hum_annotations'], v[j]['hum_annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3a57353",
   "metadata": {},
   "outputs": [],
   "source": [
    "lps = '''ENU\tFRA\n",
    "ENU\tDEU\n",
    "FRA\tENU\n",
    "DEU\tENU\n",
    "JPN\tENU\n",
    "ENU\tJPN\n",
    "ITA\tENU\n",
    "CHS\tENU\n",
    "ENU\tPTB\n",
    "ENU\tSVE\n",
    "ENU\tITA\n",
    "ENU\tDAN\n",
    "ENU\tPLK\n",
    "ARA\tENU\n",
    "ENU\tCHS\n",
    "IND\tENU\n",
    "PLK\tENU\n",
    "PTB\tENU\n",
    "ESN\tENU\n",
    "HUN\tENU\n",
    "ENU\tKOR\n",
    "CSY\tENU\n",
    "ENU\tHIN\n",
    "NLD\tENU\n",
    "KOR\tENU\n",
    "ENU\tARA\n",
    "ENU\tIND\n",
    "ENU\tNLD\n",
    "ENU\tCSY\n",
    "TRK\tENU\n",
    "ENU\tTHA\n",
    "SVE\tENU\n",
    "DAN\tENU\n",
    "ENU\tHUN\n",
    "ENU\tTRK\n",
    "ENU\tESN\n",
    "HIN\tENU\n",
    "RUS\tENU\n",
    "THA\tENU\n",
    "ENU\tRUS'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8f87e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all(pairs, test='wilcoxon'):\n",
    "    results = []\n",
    "    for df1, df2 in pairs:\n",
    "        diff = df1['Score'].mean() - df2['Score'].mean()\n",
    "        try:\n",
    "            if test == 'wilcoxon':\n",
    "                # if len(df1) != len(df2), that means there is some repeat or lost sentences\n",
    "                # look in Tom's code\n",
    "                \n",
    "                s, pvalue = scipy.stats.wilcoxon(df1['Score'].head(len(df2)), df2['Score'].head(len(df1)))\n",
    "            elif test == 'mannwhitneyu':\n",
    "                s, pvalue = scipy.stats.mannwhitneyu(df1['Score'], df2['Score'])\n",
    "            elif test == 'ttest_ind':\n",
    "                s, pvalue = scipy.stats.ttest_ind(df1['Score'].head(len(df2)), df2['Score'].head(len(df1)))\n",
    "            results.append((diff, pvalue))\n",
    "        except Exception as e:\n",
    "            results.append((0, 0.))\n",
    "            print('error', e)\n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a42ec703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENU\tFRA\t90.2944515816907\t18.24664511172684\t153\t30\t12\t3.356028680976266\n",
      "ENU\tDEU\t91.99174926180744\t14.313352089031278\t151\t19\t2\t3.5540559382666004\n",
      "FRA\tENU\t92.47588423965051\t13.612181541453348\t140\t3\t2\t2.3981481481481524\n",
      "DEU\tENU\t91.65068250646254\t14.927009713545386\t130\t27\t21\t1.6799116997792538\n",
      "JPN\tENU\t70.86299938309037\t22.90690183714039\t127\t78\t32\t3.220665499124337\n",
      "ENU\tJPN\t74.44727258576047\t21.982557300912102\t94\t40\t12\t4.480153459528154\n",
      "ITA\tENU\t88.31431698780304\t13.25638260289833\t81\t2\t3\t2.8288683850327345\n",
      "CHS\tENU\t79.39405772082303\t15.501551770980456\t78\t30\t16\t2.3196242826659557\n",
      "ENU\tPTB\t92.39283645280048\t11.026530154692553\t74\t28\t3\t4.705882352941174\n",
      "ENU\tSVE\t85.32752367548807\t19.834717905201533\t73\t31\t9\t2.1598654221836284\n",
      "ENU\tITA\t89.16265450948461\t13.866297533053874\t72\t14\t5\t3.148196590139719\n",
      "ENU\tDAN\t81.17120631199288\t18.589673563382313\t72\t24\t11\t1.7514704106280163\n",
      "ENU\tPLK\t74.66834414297048\t25.557977615077935\t71\t54\t25\t2.2176255230125435\n",
      "ARA\tENU\t80.36705116543119\t15.908248414002884\t71\t3\t4\t2.259554140127392\n",
      "ENU\tCHS\t81.14465560502482\t15.875352046537701\t70\t1\t1\t0\n",
      "IND\tENU\t82.11913488542882\t17.24337766378704\t70\t45\t15\t2.8033626275415173\n",
      "PLK\tENU\t80.73560181451244\t22.534223197187735\t70\t47\t15\t3.5759493670886116\n",
      "PTB\tENU\t93.67584770430597\t9.109210834325626\t70\t12\t3\t1.8222222222222229\n",
      "ESN\tENU\t93.33120370899731\t14.208264830780003\t70\t16\t9\t2.349264705882362\n",
      "HUN\tENU\t77.49367955759065\t24.491875871810674\t69\t27\t5\t4.765799256505574\n",
      "ENU\tKOR\t68.95445960884489\t22.284736341165686\t69\t12\t7\t4.039049235993204\n",
      "CSY\tENU\t84.78136404331899\t18.161946727019128\t69\t34\t9\t3.1528400317182133\n",
      "ENU\tHIN\t80.46944782709672\t18.729331325073833\t69\t43\t33\t2.6264110394751725\n",
      "NLD\tENU\t88.46906365300646\t15.010788331029664\t69\t35\t7\t2.2896825396825307\n",
      "KOR\tENU\t69.9213172732936\t21.349600394363932\t69\t20\t16\t2.587525150905435\n",
      "ENU\tARA\t76.79797394045256\t19.013298876355968\t68\t5\t1\t0\n",
      "ENU\tIND\t80.54556700416406\t18.109345745170437\t68\t49\t26\t2.0370942061034896\n",
      "ENU\tNLD\t86.0032181701473\t16.96669621306181\t68\t42\t19\t2.2619047619047734\n",
      "ENU\tCSY\t81.33719052445736\t19.04763974464899\t67\t35\t9\t2.7980035860518626\n",
      "TRK\tENU\t74.59162178721996\t22.427817629961208\t67\t43\t19\t3.9091027067111526\n",
      "ENU\tTHA\t77.08999792995898\t20.113290894281125\t67\t49\t14\t3.23069328896284\n",
      "SVE\tENU\t87.02654564600404\t18.632263140680895\t66\t39\t11\t4.089537347355446\n",
      "DAN\tENU\t83.639603109951\t17.13960839890452\t66\t25\t7\t2.253875968992247\n",
      "ENU\tHUN\t75.10146284429717\t25.3290299257171\t65\t36\t21\t2.4752443893707863\n",
      "ENU\tTRK\t72.49950166372764\t23.08429081735595\t65\t45\t14\t4.408639064415738\n",
      "ENU\tESN\t94.62485370691086\t12.243754518275605\t63\t12\t5\t1.9139194139194018\n",
      "HIN\tENU\t81.96094935700955\t18.568437549340068\t63\t27\t18\t3.2832618025750975\n",
      "RUS\tENU\t82.38033787436065\t14.880023571391558\t62\t11\t5\t2.507555822870472\n",
      "THA\tENU\t77.89440449556368\t18.547209215772806\t57\t53\t21\t1.4694748884099482\n",
      "ENU\tRUS\t78.97405232788726\t17.857994555223325\t52\t36\t22\t1.6814159292035384\n"
     ]
    }
   ],
   "source": [
    "for line in lps.split('\\n'):\n",
    "    records = []\n",
    "    lp_source, lp_target = tuple(line.split('\\t'))\n",
    "    records.append(lp_source)\n",
    "    records.append(lp_target)\n",
    "    \n",
    "    lp_pairs = []\n",
    "    for i, j in pairs():\n",
    "        assert(i['Source'].unique() == j['Source'].unique())\n",
    "        source = i['Source'].unique()\n",
    "        assert(len(source) == 1)\n",
    "\n",
    "        assert(i['Target'].unique() == j['Target'].unique())\n",
    "        target = i['Target'].unique()\n",
    "        assert(len(target) == 1)\n",
    "\n",
    "        if source[0] == lp_source and target[0] == lp_target:\n",
    "            lp_pairs.append((i, j))\n",
    "\n",
    "    means = []\n",
    "    for i, j in lp_pairs:\n",
    "        means.append(i['Score'].mean())\n",
    "        means.append(j['Score'].mean())\n",
    "    records.append(np.mean(means))\n",
    "\n",
    "    stddevs = []\n",
    "    for i, j in lp_pairs:\n",
    "        stddevs.append(i['Score'].std())\n",
    "        stddevs.append(j['Score'].std())\n",
    "    records.append(np.mean(stddevs))\n",
    "    \n",
    "    results = test_all(lp_pairs, test='mannwhitneyu')\n",
    "    records.append(len(results))\n",
    "    records.append(np.sum(results[:,1] < 0.05))\n",
    "    \n",
    "    order = results[:, 0].argsort()\n",
    "    sorted_results = results[order]\n",
    "\n",
    "    p_test = 0.05\n",
    "    threshold = 0.95\n",
    "    sig, insig = 0, 0\n",
    "    last_diff = 0\n",
    "    for diff, power in sorted_results:\n",
    "        if power < 0.05:\n",
    "            sig += 1\n",
    "        else:\n",
    "            insig += 1\n",
    "\n",
    "        if sig / (sig + insig) < threshold:\n",
    "            break\n",
    "        last_diff = diff\n",
    "    \n",
    "    records.append(sig + insig)\n",
    "    records.append(np.abs(last_diff))\n",
    "    \n",
    "    print('\\t'.join([str(i) for i in records ]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
